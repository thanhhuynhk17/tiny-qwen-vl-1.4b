{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23da0320-bab3-4750-a676-5e16382e4056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  9 15:38:12 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:01:00.0 Off |                  Off |\n",
      "| 81%   59C    P0            107W /  300W |       4MiB /  49140MiB |    100%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64af65-ef68-4a37-a48f-a90bb45d11ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'tiny-qwen-vl-1.4b/'\n",
      "/workspace/tiny-qwen-vl-1.4b\n",
      "Batch size: 14\n",
      "Learning rate: 2e-05\n",
      "Saving path: ./models/checkpoints\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "processor config: SiglipImageProcessor {\n",
      "  \"do_convert_rgb\": null,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"SiglipImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"processor_class\": \"SiglipProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 448,\n",
      "    \"width\": 448\n",
      "  }\n",
      "}\n",
      "\n",
      "Using device: cuda\n",
      "pixel_values\t:\ttorch.Size([14, 3, 448, 448])\n",
      "input_ids\t:\ttorch.Size([14, 291])\n",
      "attention_mask\t:\ttorch.Size([14, 291])\n",
      "labels\t:\ttorch.Size([14, 291])\n",
      "Sample pixel_values: torch.Size([3, 448, 448])\n",
      "Sample input_ids: tensor([151665, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151666,    198,   5501,  15516,\n",
      "         22986,    279,   2168,    323,   2525,    705,    448,    264,  17256,\n",
      "           369,    279,   2168,    624,     32,   1874,    315,   1251,  11699,\n",
      "          2163,    264,  22360,   1965,    304,   4065,    315,    264,  21615,\n",
      "          4171,     13, 151643])\n",
      "Sample attention_mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1])\n",
      "Sample labels: tensor([151665,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100, 151666,    198,   5501,  15516,\n",
      "         22986,    279,   2168,    323,   2525,    705,    448,    264,  17256,\n",
      "           369,    279,   2168,    624,     32,   1874,    315,   1251,  11699,\n",
      "          2163,    264,  22360,   1965,    304,   4065,    315,    264,  21615,\n",
      "          4171,     13, 151643])\n",
      "Using bfloat16\n",
      "Using device: cuda\n",
      "Missing keys: []\n",
      "Unexpected keys: ['adapter.pos_embed_cos', 'adapter.pos_embed_sin', 'adapter.key_pos_embed_cos', 'adapter.key_pos_embed_sin']\n",
      "adapter._precomputed_rope:  False\n",
      "param.requires_grad: True\n",
      "Model's stats:\n",
      " {'num_params': 1380450994, 'num_params_billion': 1.38, 'size_float16_gb': 2.761, 'size_bfloat16_gb': 2.761, 'size_float32_gb': 5.522}\n",
      "\n",
      "🚀 Epoch 1/10 Training...\n",
      "\n",
      "🟢 Step 100/2315 | Raw loss: 1.5703\n",
      "🟢 Step 200/2315 | Raw loss: 1.1719\n",
      "🟢 Step 300/2315 | Raw loss: 1.3047\n",
      "🟢 Step 400/2315 | Raw loss: 1.3594\n",
      "🟢 Step 500/2315 | Raw loss: 1.2734\n",
      "🟢 Step 600/2315 | Raw loss: 0.8516\n",
      "🟢 Step 700/2315 | Raw loss: 1.0625\n",
      "🟢 Step 800/2315 | Raw loss: 1.1250\n",
      "🟢 Step 900/2315 | Raw loss: 1.0078\n",
      "🟢 Step 1000/2315 | Raw loss: 1.1406\n",
      "🟢 Step 1100/2315 | Raw loss: 1.1641\n",
      "🟢 Step 1200/2315 | Raw loss: 1.0312\n",
      "🟢 Step 1300/2315 | Raw loss: 1.0000\n",
      "🟢 Step 1400/2315 | Raw loss: 1.1250\n",
      "🟢 Step 1500/2315 | Raw loss: 0.9375\n",
      "🟢 Step 1600/2315 | Raw loss: 1.5938\n",
      "🟢 Step 1700/2315 | Raw loss: 1.2109\n",
      "🟢 Step 1800/2315 | Raw loss: 1.1484\n",
      "🟢 Step 1900/2315 | Raw loss: 0.8828\n",
      "🟢 Step 2000/2315 | Raw loss: 1.1797\n",
      "🟢 Step 2100/2315 | Raw loss: 0.8711\n",
      "🟢 Step 2200/2315 | Raw loss: 0.9766\n",
      "🟢 Step 2300/2315 | Raw loss: 1.0000\n",
      "\n",
      "📊 Epoch 1 | Avg Loss: 1.1390\n",
      "\n",
      "📊 Validation Results | Loss: 1.0058\n",
      "✅ New best checkpoint saved ./models/checkpoints/best_tinyqwenvl_1.4B-phase2.pth 1.0058\n",
      "\n",
      "🚀 Epoch 2/10 Training...\n",
      "\n",
      "🟢 Step 100/2315 | Raw loss: 0.8594\n",
      "🟢 Step 200/2315 | Raw loss: 1.0000\n",
      "🟢 Step 300/2315 | Raw loss: 1.0391\n",
      "🟢 Step 400/2315 | Raw loss: 0.9688\n",
      "🟢 Step 500/2315 | Raw loss: 0.9375\n",
      "🟢 Step 600/2315 | Raw loss: 1.0547\n",
      "🟢 Step 700/2315 | Raw loss: 0.9492\n",
      "🟢 Step 800/2315 | Raw loss: 0.8945\n",
      "🟢 Step 900/2315 | Raw loss: 0.9922\n",
      "🟢 Step 1000/2315 | Raw loss: 0.9688\n",
      "🟢 Step 1100/2315 | Raw loss: 0.8633\n",
      "🟢 Step 1200/2315 | Raw loss: 1.0234\n",
      "🟢 Step 1300/2315 | Raw loss: 0.9141\n",
      "🟢 Step 1400/2315 | Raw loss: 1.1094\n",
      "🟢 Step 1500/2315 | Raw loss: 0.9180\n",
      "🟢 Step 1600/2315 | Raw loss: 1.0547\n",
      "🟢 Step 1700/2315 | Raw loss: 1.0000\n",
      "🟢 Step 1800/2315 | Raw loss: 0.8633\n",
      "🟢 Step 1900/2315 | Raw loss: 0.9805\n",
      "🟢 Step 2000/2315 | Raw loss: 0.8906\n",
      "🟢 Step 2100/2315 | Raw loss: 1.0156\n",
      "🟢 Step 2200/2315 | Raw loss: 0.9727\n",
      "🟢 Step 2300/2315 | Raw loss: 0.8125\n",
      "\n",
      "📊 Epoch 2 | Avg Loss: 0.9564\n",
      "\n",
      "📊 Validation Results | Loss: 0.9525\n",
      "✅ New best checkpoint saved ./models/checkpoints/best_tinyqwenvl_1.4B-phase2.pth 0.9525\n",
      "\n",
      "🚀 Epoch 3/10 Training...\n",
      "\n",
      "🟢 Step 100/2315 | Raw loss: 0.9453\n",
      "🟢 Step 200/2315 | Raw loss: 0.9688\n",
      "🟢 Step 300/2315 | Raw loss: 1.1875\n",
      "🟢 Step 400/2315 | Raw loss: 0.9492\n",
      "🟢 Step 500/2315 | Raw loss: 0.7109\n",
      "🟢 Step 600/2315 | Raw loss: 0.8438\n",
      "🟢 Step 700/2315 | Raw loss: 0.8516\n"
     ]
    }
   ],
   "source": [
    "%cd tiny-qwen-vl-1.4b/\n",
    "!torchrun --nproc_per_node=1 src/train-phase2.py \\\n",
    "    --batch_size 14 --learning_rate 2e-5 --save_path ./models/checkpoints \\\n",
    "    --ckpt_path ./models/checkpoints/best_tinyqwenvl_1.4B.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e17b78-0512-4eb3-8f8a-644a8b3e1a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab32499-b887-47a3-9d81-d6292cdb9af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
